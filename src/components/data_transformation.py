import os
import sys
import pandas as pd
import numpy as np

from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split

from src.logging import get_logger
from src.exception import CustomException
from src.utils.config import DataTransformationConfig

logging = get_logger(__name__)

class DataTransformation:
    def __init__(self):
        """
        Initiating the Data Transformation Configuration
        1) Loading the dataset.
        2) Converting the target column from 'yes'/'no' to 1/0.
        3) Separating the features into numerical and categorical columns.
        4) Building preprocessing pipelines for numeric and categorical data.
        5) Combining pipelines using ColumnTransformer.
        6) Fitting and transforming the data.
        7) Performing feature selection.
        8) Splitting the data into train and test sets.
        9) Saving the final transformed datasets.
        """
        self.data_transformation_config = DataTransformationConfig()

    def initialising_data_transformation(self):
        try:
            # 1) Load dataset from the specified input path.
            df = pd.read_csv(self.data_transformation_config.input_path)
            logging.info('Loading the Business Marketing CSV dataset')

            target_column = 'deposit'
            # 2) Convert target values to binary (yes -> 1, no -> 0).
            df[target_column] = df[target_column].map({"yes": 1, "no": 0})
            
            # Separate the dataset into features (X) and target (y).
            X = df.drop(columns=[target_column])
            y = df[target_column]

            # 3) Define lists of numerical and categorical columns.
            numerical_columns = [
                'age',
                'balance',
                'day',
                'duration',
                'campaign',
                'pdays',
                'previous'
                ]
            categorical_columns =[
                'job',
                'marital',
                'education',
                'default',
                'housing',
                'loan',
                'contact',
                'month',
                'poutcome',
                ]
            
            # 4) Build a pipeline for numerical features:
            #    - Impute missing values using the median.
            #    - Scale the features using StandardScaler.
            numerical_pipeline = Pipeline(
                steps=[
                    ("imputer",SimpleImputer(strategy="median")), ## Handling Missing Values
                    ("scaler",MinMaxScaler()),  # Use MinMaxScaler to keep values non-negative
                ]
            )
            # 5) Build a pipeline for categorical features:
            #    - Impute missing values using the most frequent value.
            #    - One-hot encode categorical features.
            categorical_pipeline = Pipeline(
                steps=[
                    ("imputer", SimpleImputer(strategy="most_frequent")),
                    ("OneHotEncoder", OneHotEncoder())
                ]
            )

            # 6) Combine both pipelines into a single ColumnTransformer.
            preprocessor = ColumnTransformer(
                transformers=[
                    ("num_pipeline", numerical_pipeline, numerical_columns),
                    ("cat_pipeline", categorical_pipeline, categorical_columns)
                ]
            )

            # 7) Fit the preprocessor on the features and transform the data.
            X_transformed = preprocessor.fit_transform(X)

            # Retrieve column names generated by the OneHotEncoder for categorical features.
            ohe = preprocessor.named_transformers_["cat_pipeline"].named_steps["OneHotEncoder"]
            ohe_feature_names = list(ohe.get_feature_names_out(categorical_columns))

            # Combine numeric columns with one-hot encoded categorical column names.
            final_columns = numerical_columns + ohe_feature_names

            # 8) Construct a new DataFrame with the transformed features.
            X_transformed_df = pd.DataFrame(X_transformed, columns=final_columns)

            # Reattach the target column to the transformed DataFrame.
            X_transformed_df[target_column] = y.values
            logging.info("Feature Engineering Completed")


             # 10) Split the dataset (with selected features) into train and test sets.
            train_df, test_df = train_test_split(
                X_transformed_df,
                test_size=0.2,
                random_state=42,
                stratify=X_transformed_df[target_column],
            )
            logging.info("Dataset split into train and test sets.")

             # 11) Save the final train and test CSV files for downstream processes.
            train_df.to_csv(self.data_transformation_config.train_path, index=False)
            test_df.to_csv(self.data_transformation_config.test_path, index=False)

            logging.info(f"Train dataset saved to {self.data_transformation_config.train_path}")
            logging.info(f"Test dataset saved to {self.data_transformation_config.test_path}")

            return (
                self.data_transformation_config.train_path,
                self.data_transformation_config.test_path
            )
        except Exception as e:
            raise CustomException(e,sys)

if __name__ == "__main__":
    transformation = DataTransformation()
    transformed_file = transformation.initialising_data_transformation()



